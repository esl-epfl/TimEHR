{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activate line execution\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# general\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# import custom libraries\n",
    "import sys\n",
    "import os\n",
    "import tqdm\n",
    "import pickle\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# plotly\n",
    "import plotly.express as px  # (version 4.7.0 or higher)\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_datasets = ['p12','p19','mimic','sim-l0.5-d16']\n",
    "\n",
    "DATASET = 'sim-l0.5-d16'\n",
    "\n",
    "f = yaml.safe_load(open(f'../configs/data/{DATASET}.yaml'))\n",
    "\n",
    "PATH_RAW = f['path_raw']\n",
    "PATH_PROCESSED = f['path_processed']\n",
    "# create path_processed if it does not exist\n",
    "if not os.path.exists(PATH_PROCESSED):\n",
    "    os.makedirs(PATH_PROCESSED)\n",
    "\n",
    "\n",
    "print(f\"DATASET: {DATASET}\")\n",
    "print(f\"PATH_RAW: {PATH_RAW}\")\n",
    "print(f\"PATH_PROCESSED: {PATH_PROCESSED}\")\n",
    "\n",
    "# This will shuffle the order of clinical time series in the image\n",
    "SUFFLE_VARS = True\n",
    "\n",
    "\n",
    "# for TimEHR format\n",
    "GRAN=1 # granularity of the image (in hours)\n",
    "IMG_SIZE = f['img_size'] # granularity of the image\n",
    "N_SPLIT = 5 # number of splits for the cross-validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int(np.log2(64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_demo(df_demo, demo_vars):\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    demo_vars\n",
    "\n",
    "    # print(demo_vars)\n",
    "    # print(df.columns)\n",
    "\n",
    "    # df_demo = df[['RecordID','Hospital']+demo_vars].dropna(subset='Age')\n",
    "    # df_demo = df[['RecordID','Hospital']+demo_vars].dropna()\n",
    "    # print(df_demo.columns)\n",
    "    # term\n",
    "    df_demo\n",
    "\n",
    "    # replace -1 with NaN and the mean of each column\n",
    "    df_demo.describe()\n",
    "    df_demo[df_demo==-1]=np.nan\n",
    "    df_demo.isnull().sum()\n",
    "    df_demo.describe()\n",
    "    for column in df_demo.columns:\n",
    "        df_demo[column] = df_demo[column].fillna(df_demo[column].mean())\n",
    "        # print(column)\n",
    "    df_demo.isnull().sum()\n",
    "    df_demo.describe()\n",
    "\n",
    "    \n",
    "\n",
    "    # standardize continuous variables\n",
    "    demo_statistics={}\n",
    "    for column in demo_vars:\n",
    "        if column in ['Age', 'Height','Weight','HospAdmTime']:\n",
    "            demo_statistics[column] = {'mean':df_demo[column].mean(),'std':df_demo[column].std()}\n",
    "\n",
    "            df_demo[column] = (df_demo[column]-df_demo[column].mean())/df_demo[column].std()\n",
    "\n",
    "\n",
    "    # discritze ICUType\n",
    "    if 'ICUType' in demo_vars:\n",
    "\n",
    "        ohe = OneHotEncoder()\n",
    "        transformed = ohe.fit_transform(df_demo[['ICUType']])\n",
    "        transformed\n",
    "        mat_ICU_enc = pd.DataFrame.sparse.from_spmatrix(transformed).values.astype(int)\n",
    "        new_cols=['ICUType'+str(i) for i in range(mat_ICU_enc.shape[1])]\n",
    "        df_demo[new_cols]=mat_ICU_enc\n",
    "\n",
    "        df_demo = df_demo.drop(columns='ICUType')\n",
    "    \n",
    "    if 'Gender' in demo_vars:\n",
    "        df_demo['Gender'] = df_demo['Gender'].astype(int)  \n",
    "         \n",
    "\n",
    "    # df_demo.rename(columns={'RecordID':'id'},inplace=True)\n",
    "\n",
    "    df_demo\n",
    "\n",
    "    demo_vars_enc = list(df_demo.columns)\n",
    "    demo_vars_enc.remove('RecordID')\n",
    "    if 'Hospital' in demo_vars_enc:\n",
    "        demo_vars_enc.remove('Hospital')\n",
    "    \n",
    "    print(demo_vars_enc)\n",
    "    print(df_demo)\n",
    "    df_demo['dict_demo'] = df_demo[demo_vars_enc].apply(lambda x:list(x),axis=1)\n",
    "\n",
    "    df_demo.iloc[0]['dict_demo']\n",
    "\n",
    "\n",
    "    dict_map_demos = {k:i for i,k in enumerate(demo_vars_enc)}\n",
    "    dict_map_demos\n",
    "\n",
    "    return df_demo, dict_map_demos, demo_statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_process(df_demo, df_filt, train_ids,state_vars, conditional_vars):\n",
    "\n",
    "    df_demo_train = df_demo[df_demo['RecordID'].isin(train_ids)].copy()\n",
    "    df_filt_train = df_filt[df_filt['RecordID'].isin(train_ids)].copy()\n",
    "\n",
    "    df_demo_test = df_demo[~df_demo['RecordID'].isin(train_ids)].copy()\n",
    "    df_filt_test = df_filt[~df_filt['RecordID'].isin(train_ids)].copy()\n",
    "\n",
    "    # if test is empty\n",
    "    if df_demo_test.shape[0]==0:\n",
    "        df_demo_test = df_demo_train.copy()\n",
    "        df_filt_test = df_filt_train.copy()\n",
    "    print('1',df_filt_train.isnull().sum().sum())\n",
    "\n",
    "    # Normalize state variables\n",
    "\n",
    "    # 1] normalized cols in state_vars from df_filt\n",
    "    print('Step 1: Normalization ',df_filt_train.isnull().sum().sum())\n",
    "\n",
    "    state_preprocess = {'mean':df_filt_train[state_vars].mean(),'std':df_filt_train[state_vars].std()}\n",
    "\n",
    "    df_filt_train[state_vars] = (df_filt_train[state_vars]- state_preprocess['mean'])/state_preprocess['std']\n",
    "    df_filt_test[state_vars] = (df_filt_test[state_vars]- state_preprocess['mean'])/state_preprocess['std']\n",
    "\n",
    "    # 2] set outliers to nan\n",
    "    print('Step 2: set outliers to nan ',df_filt_train.isnull().sum().sum())\n",
    "    if DATASET not in ['energy','stock']:\n",
    "        df_filt_train[state_vars] = df_filt_train[state_vars].apply(lambda x: x.mask(x.sub(x.mean()).div(x.std()).abs().gt(3)))\n",
    "        df_filt_test[state_vars] = df_filt_test[state_vars].apply(lambda x: x.mask(x.sub(x.mean()).div(x.std()).abs().gt(3)))\n",
    "        # print('step 2',df_filt_train.isnull().sum().sum())\n",
    "        \n",
    "    # 3] now do min-max normalization # between 0 and 1\n",
    "    print('Step 3: min-max normalization ')\n",
    "    state_preprocess['min'] = df_filt_train[state_vars].min()\n",
    "    state_preprocess['max'] = df_filt_train[state_vars].max()\n",
    "\n",
    "    df_filt_train[state_vars] = (df_filt_train[state_vars]-state_preprocess['min'])/(state_preprocess['max']-state_preprocess['min'])\n",
    "    df_filt_test[state_vars] = (df_filt_test[state_vars]-state_preprocess['min'])/(state_preprocess['max']-state_preprocess['min'])\n",
    "\n",
    "    # # # 4] scale to [-1,1]\n",
    "    # print('Step 4: scale to [-1,1] ')\n",
    "    # df_filt_train[state_vars] = df_filt_train[state_vars]*2-1\n",
    "    # df_filt_test[state_vars] = df_filt_test[state_vars]*2-1\n",
    "\n",
    "\n",
    "    # Normalize demo variables\n",
    "    print('Step 5: Normalize demo variables ')\n",
    "    df_demo_train, demo_dict,demo_preprocess = create_df_demo(df_demo_train, conditional_vars)\n",
    "\n",
    "    demo_preprocess['demo_vars_enc'] = list(demo_dict.keys())\n",
    "    df_demo_test, _,_ = create_df_demo(df_demo_test, conditional_vars)\n",
    "    \n",
    "    # print(list(demo_preprocess['demo_vars_enc']))\n",
    "    return df_demo_train, df_demo_test, df_filt_train, df_filt_test, state_preprocess, demo_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_cgan(df_demo, df_filt, state_vars,demo_vars_enc,granularity=1,IMG_SIZE=64):\n",
    "   # var_old = [ 'Albumin', 'ALP', 'ALT', 'AST', 'Bilirubin', 'BUN',\n",
    "   #    'Cholesterol', 'Creatinine', 'DiasABP', 'FiO2', 'GCS', 'Glucose',\n",
    "   #    'HCO3', 'HCT', 'HR', 'K', 'Lactate', 'Mg', 'MAP', 'MechVent', 'Na',\n",
    "   #    'NIDiasABP', 'NIMAP', 'NISysABP', 'PaCO2', 'PaO2', 'pH', 'Platelets',\n",
    "   #    'RespRate', 'SaO2', 'SysABP', 'Temp', 'TroponinI', 'TroponinT', 'Urine',\n",
    "   #    'WBC']\n",
    "\n",
    "   # print('set(state_vars)-set(var_old) => ',set(state_vars)-set(var_old))\n",
    "   # print('set(var_old)-set(state_vars) => ',set(var_old)-set(state_vars))\n",
    "\n",
    "\n",
    "   # df_demo, demo_dict = create_df_demo(df_demo, demo_vars)\n",
    "   # print(demo_dict)\n",
    "   # print(df_demo.columns)\n",
    "   print(demo_vars_enc)\n",
    "   sta = df_demo[demo_vars_enc].rename(columns={'In-hospital_death':'Label'})\n",
    "   # sta['Label'] = sta['Label'].astype(float)\n",
    "   # sta[\"Gender\"] = sta[\"Gender\"].astype(int)\n",
    "   # sta[\"Label\"] = sta[\"Label\"].astype(int)\n",
    "\n",
    "   # print(sta)\n",
    "   all_sta = torch.from_numpy(sta.values)\n",
    "   print(all_sta.shape)\n",
    "\n",
    "\n",
    "\n",
    "   dyn=[]\n",
    "\n",
    "   grouped = df_filt.groupby('RecordID')\n",
    "\n",
    "   # Create a list of DataFrames\n",
    "   dyn = [group[   ['Time']+state_vars].rename(columns={'Time':'time'}) for _, group in grouped]\n",
    "\n",
    "   all_times = []\n",
    "   all_dyn = []\n",
    "   all_masks = []\n",
    "   all_time_mark = []\n",
    "   for SAMPLE in dyn:\n",
    "      time = SAMPLE.time.values\n",
    "      # times_padded = pd.DataFrame({'time':np.arange(0,   int(max(time)),granularity)})\n",
    "\n",
    "      # print(time,len(time),max(time))\n",
    "      # print(times_padded.values.flatten(),len(times_padded.values.flatten()))\n",
    "      \n",
    "      # create a np array from 0 to max(time) with granularity 0.5 ()including max(time)) use linspace\n",
    "      time_padded = np.linspace(0, int(IMG_SIZE/granularity)-granularity, IMG_SIZE) # shape is (IMG_SIZE,)\n",
    "      time_mark = time_padded<=max(time)\n",
    "      # print(time_mark,time_mark.shape)\n",
    "      # term\n",
    "      df_time_padded = pd.DataFrame({'time':time_padded})\n",
    "      temp = df_time_padded.merge(SAMPLE,how='outer',on='time')#.sort_values(by='time_padded').\n",
    "      \n",
    "      dyn_padded = temp[state_vars].fillna(0).values\n",
    "      mask_padded = temp[state_vars].notnull().astype(int).values\n",
    "\n",
    "\n",
    "      all_times.append(torch.from_numpy(time_padded))\n",
    "      all_dyn.append(torch.from_numpy(dyn_padded))\n",
    "      all_masks.append(torch.from_numpy(mask_padded))\n",
    "      all_time_mark.append(torch.from_numpy(time_mark))\n",
    "      # print(time_padded.shape,dyn_padded.shape,mask_padded.shape)\n",
    "   \n",
    "      \n",
    "   all_masks = torch.stack(all_masks, dim=0)\n",
    "   all_dyn = torch.stack(all_dyn, dim=0)\n",
    "   all_times = torch.stack(all_times, dim=0)\n",
    "   all_time_mark =torch.stack(all_time_mark)\n",
    "   # PADDING\n",
    "   # IMG_SIZE = IMG_SIZE\n",
    "   padding_needed = IMG_SIZE - all_masks.shape[-1]\n",
    "\n",
    "   all_masks_padded = torch.nn.functional.pad(all_masks, (0, padding_needed)).unsqueeze(1).float() # add channel dim\n",
    "   all_dyn_padded = torch.nn.functional.pad(all_dyn, (0, padding_needed)).unsqueeze(1).float() # add channel dim\n",
    "   all_times_padded = torch.nn.functional.pad(all_times, (0, padding_needed))\n",
    "   all_masks_padded.shape\n",
    "   all_dyn_padded.shape\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "   all_data = torch.stack([all_dyn_padded, all_masks_padded], dim=1)\n",
    "   all_data.shape\n",
    "   \n",
    "   # # SCALE all_data to [-1,1]\n",
    "   all_masks_padded = all_masks_padded*2-1\n",
    "   all_dyn_padded = all_dyn_padded*2-1\n",
    "   all_dyn_padded[all_masks_padded<0]=0\n",
    "   # all_data = all_data*2-1\n",
    "   # all_data[:,0,:,:][all_data[:,1,:,:]<0]=0\n",
    "\n",
    "\n",
    "   \n",
    "   return all_masks_padded.float(),all_dyn_padded.float(),all_sta.float(), all_time_mark.int()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processed Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if DATASET=='p12':\n",
    "    print('### Handling P12 dataset')\n",
    "    df = pd.read_csv(PATH_RAW+f'/{DATASET}.csv', index_col=None)\n",
    "    \n",
    "    print('# Number of patients:',df.RecordID.nunique())\n",
    "\n",
    "    # convert time to hours\n",
    "    df['Time'] = df['Time'].apply(lambda x:       int(x.split(':')[0]) + int(x.split(':')[1])/60      )\n",
    "\n",
    "    # rename column\n",
    "    df.rename(columns={'In-hospital_death':'Label'}, inplace=True)\n",
    "    \n",
    "    print('# Prevelence of in-hospital mortality: ', df.Label.mean())\n",
    "\n",
    "    # columns handling\n",
    "    all_cols = df.columns\n",
    "\n",
    "    cols_id = ['RecordID','Time']\n",
    "    cols_outcome = ['Label']\n",
    "    cols_demo = ['Age','Gender','Height','ICUType','Weight']\n",
    "    cols_vital = ['HR','NIDiasABP', 'NIMAP', 'NISysABP','RespRate', 'Temp','DiasABP','MAP','SysABP', 'GCS']\n",
    "    cols_lab = [ 'BUN', 'Creatinine', 'Glucose', 'HCO3', 'HCT', 'K', 'Mg','Na', 'Platelets','Urine', 'WBC','FiO2', 'PaCO2', 'PaO2', 'SaO2', 'pH', 'ALP', 'ALT',\n",
    "        'AST', 'Albumin', 'Bilirubin', 'Lactate', 'Cholesterol', 'TroponinI',\n",
    "        'TroponinT']\n",
    "    # cols_ignore = ['MechVent','Hospital'] # will be ignored\n",
    "\n",
    "    print('# Distribution of LOS:')\n",
    "    print(df.groupby('RecordID')['Time'].max().describe())\n",
    "\n",
    "\n",
    "    \n",
    "elif DATASET in ['p19']:\n",
    "    print('### Handling P19 dataset')\n",
    "\n",
    "\n",
    "    # data from hispital A and B\n",
    "    df_A = pd.read_csv(PATH_RAW+'/df_A.csv')#.merge(df_filenames_A,on='id')\n",
    "    df_A['Hospital']=0\n",
    "\n",
    "    df_B = pd.read_csv(PATH_RAW+'/df_B.csv')#.merge(df_filenames_B,on='id')\n",
    "    df_B['Hospital']=1\n",
    "\n",
    "    # combine two datasets into one\n",
    "    df_B['id']=df_B['id']+len(df_A['id'].unique())\n",
    "    df=pd.concat([df_A,df_B])\n",
    "\n",
    "    print('# Number of patients:',df.id.nunique())\n",
    "    \n",
    "    pos_ids = df[df.SepsisLabel==1].id.unique()    \n",
    "\n",
    "    print('# Prevelence of sepsis:',len(pos_ids)/df.id.nunique())\n",
    "    # IMPORTANT\n",
    "    # we shift the label 6 hours before. This is because the label at each row indicates sepsis for the next 6 hours\n",
    "    df['SepsisLabel'] = df.groupby('id')['SepsisLabel'].shift(6, fill_value=0).astype(int)\n",
    "\n",
    "    # we remove the rows with label 1. Our goal is to predict sepsis patients, not the detection of onset of sepsis   \n",
    "    df = df[df.SepsisLabel!=1]\n",
    "\n",
    "    # if patient is septic, we set the label to 1 across all rows\n",
    "    df.loc[df.id.isin(pos_ids),'SepsisLabel'] = 1\n",
    "    df.loc[~df.id.isin(pos_ids),'SepsisLabel'] = 0\n",
    "    \n",
    "\n",
    "    # rename some columns\n",
    "    df.rename(columns={'id':'RecordID','ICULOS':'Time','SepsisLabel':'Label'}, inplace=True)\n",
    "\n",
    "    print('# Distribution of LOS: brefore removing patients with LOS>64')\n",
    "    print('# df.shape:',df.shape)\n",
    "    print(df.groupby('RecordID')['Time'].max().describe())\n",
    "    # df['Time'].hist(bins=100)\n",
    "    \n",
    "    # we only keep first 64 hours\n",
    "    df = df[df.Time<=63]\n",
    "\n",
    "    print('# Distribution of LOS: after removing patients with LOS>64')\n",
    "    print('# df.shape:',df.shape)\n",
    "    print(df.groupby('RecordID')['Time'].max().describe())\n",
    "    # df['Time'].hist(bins=100)\n",
    "\n",
    "    \n",
    "    # columns handling\n",
    "    all_cols = df.columns\n",
    "\n",
    "    cols_id = ['RecordID','Time']\n",
    "    cols_outcome = ['Label']\n",
    "    cols_demo = ['Age','Gender','HospAdmTime']\n",
    "    cols_vital = [ 'HR', 'O2Sat', 'Temp', 'SBP', 'MAP', 'DBP', 'Resp'] \n",
    "    cols_lab = [ \n",
    "        'BUN','Creatinine', 'Glucose',  'HCO3', 'Potassium','Magnesium',\n",
    "        'Hct','Platelets','WBC',\n",
    "        'FiO2', 'PaCO2','pH','SaO2',        \n",
    "         'Alkalinephos','AST', 'Bilirubin_total','Bilirubin_direct',\n",
    "         'Lactate',  'TroponinI',\n",
    "        'Hgb', 'Chloride', \n",
    "          'Phosphate', 'Calcium',    'PTT',   'Fibrinogen' ]\n",
    "    # cols_ignore = ['EtCO2','BaseExcess','Unit1', 'Unit2','Hospital'] # will be ignored\n",
    "\n",
    "elif DATASET in ['mimic']:\n",
    "\n",
    "    df = pd.read_csv(PATH_RAW+'/df_mimic.csv', index_col=None)\n",
    "\n",
    "    # columns handling\n",
    "    all_cols = df.columns\n",
    " \n",
    "    cols_id = ['RecordID','Time']\n",
    "    cols_outcome = ['Label']\n",
    "    cols_demo = ['Age','Height','Weight']\n",
    "    cols_vital = ['heartrate', 'sysbp', 'diasbp', 'meanbp',\n",
    "        'resprate', 'tempc', 'spo2']\n",
    "    cols_lab = [ 'glucose_chart',  'endotrachflag', 'bg_so2', 'bg_po2', 'bg_pco2',\n",
    "        'bg_pao2fio2ratio', 'bg_ph', 'bg_baseexcess', 'bg_bicarbonate',\n",
    "        'bg_totalco2', 'bg_hematocrit', 'bg_hemoglobin', 'bg_carboxyhemoglobin',\n",
    "        'bg_methemoglobin', 'bg_chloride', 'bg_calcium', 'bg_temperature',\n",
    "        'bg_potassium', 'bg_sodium', 'bg_lactate', 'bg_glucose', 'aniongap',\n",
    "        'albumin', 'bands', 'bicarbonate', 'bilirubin', 'creatinine',\n",
    "        'chloride', 'glucose', 'hematocrit', 'hemoglobin', 'lactate',\n",
    "        'platelet', 'potassium', 'ptt', 'inr', 'pt', 'sodium', 'bun', 'wbc',\n",
    "        'urineoutput']\n",
    "    # cols_ignore = []\n",
    "\n",
    "elif 'sim' in DATASET:\n",
    "    \n",
    "    df = pd.read_csv(PATH_RAW+f'/{DATASET}.csv')\n",
    "\n",
    "    if '16' in DATASET:\n",
    "        seq_len = 16\n",
    "    elif '32' in DATASET:\n",
    "        seq_len = 32\n",
    "    elif '64' in DATASET:\n",
    "        seq_len = 64\n",
    "    elif '128' in DATASET:\n",
    "        seq_len = 128\n",
    "    # seq_len = 128 if '128' in DATASET else 64\n",
    "\n",
    "    \n",
    "\n",
    "    # MODE 1: disjoint\n",
    "\n",
    "    a = len(df)//seq_len*seq_len\n",
    "    df = df.iloc[:a]\n",
    "    # simplify above\n",
    "\n",
    "    var_names = df.columns.tolist()\n",
    "\n",
    "    df['Time'] =  np.tile(np.arange(seq_len),len(df)//seq_len)\n",
    "    df['RecordID'] = np.repeat(np.arange(len(df)//seq_len),seq_len)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # # MODE 2: overlapping\n",
    "    # mat = df.values\n",
    "    # # Preprocess the dataset\n",
    "    # temp_data = []\n",
    "    # id_data = []\n",
    "    # time_data = []\n",
    "    # # Cut data by sequence length\n",
    "    # for i in range(0, len(mat) - seq_len):\n",
    "    #     _x = mat[i:i + seq_len]\n",
    "    #     temp_data.append(_x)\n",
    "    #     id_data.append(np.array(np.ones(seq_len)*i))\n",
    "    #     time_data.append(np.arange(seq_len))\n",
    "    # len(temp_data),temp_data[0].shape\n",
    "\n",
    "    # mat2 = np.concatenate(temp_data)\n",
    "    # mat2.shape\n",
    "\n",
    "    # id_data = np.concatenate(id_data)\n",
    "    # id_data.shape\n",
    "    \n",
    "    # time_data = np.concatenate(time_data)\n",
    "    # time_data.shape\n",
    "    #     #form a dataframe\n",
    "    # df = pd.DataFrame(mat2, columns=df.columns)\n",
    "    # df['RecordID'] = id_data\n",
    "    # df['Time'] = time_data\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    # continue\n",
    "    \n",
    "    df['Hospital'] = 0\n",
    "    # set age randomly\n",
    "    df['Age'] = np.random.randint(20,80,len(df))     \n",
    "    df['Label'] = 0\n",
    "    all_cols = df.columns\n",
    "\n",
    "    cols_id = ['RecordID','Hospital','Time']\n",
    "    cols_outcome = ['Label']\n",
    "    cols_demo = ['Age']\n",
    "    cols_vital = var_names\n",
    "    cols_lab = []\n",
    "    cols_ignore = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_cols = list(set(all_cols) - set(cols_id) - set(cols_outcome) - set(cols_demo) - set(cols_vital) - set(cols_lab) )\n",
    "\n",
    "\n",
    "print(\"remaining_cols: \", remaining_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_vars = cols_vital+cols_lab\n",
    "\n",
    "if SUFFLE_VARS:\n",
    "    np.random.seed(42)\n",
    "    np.random.shuffle(state_vars)\n",
    "\n",
    "\n",
    "demo_vars = cols_demo\n",
    "\n",
    "dict_map_states = {label:i for i,label in enumerate(state_vars)}\n",
    "\n",
    "dict_map_demos = {k:i for i,k in enumerate(cols_demo)}\n",
    "dict_map_demos\n",
    "\n",
    "\n",
    "print(\"number of state variables (vital+lab): \", len(state_vars))\n",
    "print(\"number of demographic variables: \", len(demo_vars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split df to df_ts and df_static\n",
    "\n",
    "df['RecordID'] = df['RecordID'].astype(int)\n",
    "df['Label'] = df['Label'].astype(int)\n",
    "\n",
    "df_ts = df[['RecordID','Time']+cols_vital+cols_lab].copy()\n",
    "df_static = df[['RecordID']+cols_demo+cols_outcome].drop_duplicates(subset='RecordID').copy()\n",
    "\n",
    "df_ts.shape, df_static.shape\n",
    "\n",
    "df_ts.head()\n",
    "df_static.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for missing rate\n",
    "\n",
    "df_ts.isnull().sum()/(df_ts.shape[0]*df_ts.shape[1])*100\n",
    "\n",
    "df_static.isnull().sum()/(df_static.shape[0]*df_static.shape[1])*100\n",
    "\n",
    "print(\"overall missingness rate(%): \", df_ts.isnull().sum().sum()/(df_ts.shape[0]*df_ts.shape[1])*100)\n",
    "\n",
    "# bar plot of missing rate for each variable using plotly\n",
    "\n",
    "missing_rate = df_ts.isnull().mean().sort_values(ascending=False)\n",
    "missing_rate = missing_rate[missing_rate>0]\n",
    "\n",
    "fig = px.bar(x=missing_rate.index, y=missing_rate.values*100, labels={'x':'Variable','y':'Missing rate (%)'}, title='Missing rate for each variable')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose time granularity\n",
    "time_granularity = 1  # 1 hour\n",
    "\n",
    "df_ts.iloc[:30].Time.values\n",
    "df_ts['Time'] = df_ts['Time'].apply(lambda x: round(x/time_granularity,0)*time_granularity)\n",
    "df_ts.iloc[:30].Time.values\n",
    "df_ts.shape\n",
    "df_ts.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows if all time series variables are missing\n",
    "df_ts.shape\n",
    "\n",
    "df_ts = df_ts.dropna(subset=cols_vital+cols_lab, how='all')\n",
    "\n",
    "df_ts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ts.head(10)\n",
    "\n",
    "df_ts.describe()\n",
    "\n",
    "# You can see that in P12 we might have multiple measurements for the same time point.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate\n",
    "\n",
    "if multiple observation then ffill and keep last\n",
    "\n",
    "It is only used for P12 dataset. Time granularity for other datasets is already 1 hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is aggregation needed?\n",
    "\n",
    "a1 = df_ts.groupby('RecordID').size().values\n",
    "a2 = df_ts.groupby('RecordID')['Time'].nunique().values\n",
    "\n",
    "if (a1-a2).sum()>0:\n",
    "    print('df.shape before aggregation:',df_ts.shape)\n",
    "    # forwardfill for each group\n",
    "\n",
    "    df_ts[cols_lab+cols_vital] = df_ts[['RecordID','Time']+cols_lab+cols_vital].groupby(['RecordID','Time']).fillna(method='ffill')\n",
    "\n",
    "    # keep last for each group\n",
    "    df_ts = df_ts[['RecordID','Time']+cols_lab+cols_vital].groupby(['RecordID','Time']).last().reset_index()\n",
    "    print('df.shape after aggregation:',df_ts.shape)\n",
    "else:\n",
    "    print('no aggregation needed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_static.shape\n",
    "\n",
    "# forwarfill for each group\n",
    "df_static[cols_demo+cols_outcome] = df_static[['RecordID']+cols_demo+cols_outcome].groupby(['RecordID']).fillna(method='ffill')\n",
    "\n",
    "# keep last for each group\n",
    "df_static = df_static[['RecordID']+cols_demo+cols_outcome].groupby(['RecordID']).last().reset_index()\n",
    "\n",
    "df_static.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ts.RecordID.nunique(), df_static.RecordID.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ts.Time.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-fold\n",
    "\n",
    "this will write df_ts and df_static for each split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SPLIT=5 # number of folds\n",
    "\n",
    "# seed for numpy and dataframe sampling\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "# shuffle ids\n",
    "list_ids = df_static['RecordID'].values.copy()\n",
    "np.random.shuffle(list_ids)\n",
    "list_ids[:5]\n",
    "\n",
    "df_static = df_static[df_static['RecordID'].isin(list_ids)]\n",
    "df_static.head()\n",
    "\n",
    "\n",
    "\n",
    "split_list = np.linspace(0,len(df_static),5+1).astype(int)\n",
    "split_list\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i_split in range(N_SPLIT):\n",
    "\n",
    "    path2save = PATH_PROCESSED+f'/split{i_split}'\n",
    "    os.makedirs(path2save , exist_ok=True)\n",
    "    \n",
    "    test_ids = list_ids[split_list[i_split]:split_list[i_split+1]]\n",
    "    i_split, sum(test_ids)\n",
    "    train_ids = list(set(list_ids)-set(test_ids))\n",
    "\n",
    "        \n",
    "    # save train ids\n",
    "    with open(path2save+'/train_ids.pkl', 'wb') as f:\n",
    "        pickle.dump(train_ids, f)\n",
    "\n",
    " \n",
    "\n",
    "    # save df_static and df_ts\n",
    "    df_static.to_csv(path2save+'/df_static.csv',index=False)\n",
    "    df_ts.to_csv(path2save+'/df_ts.csv',index=False)\n",
    "    \n",
    "    print('saved to', path2save)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# 163575645"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TimEHR format\n",
    "\n",
    "We further process df_ts and df_static to images for TimEHR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class\n",
    "\n",
    "class Physio3(Dataset):\n",
    "    def __init__(self, all_masks,all_values,all_sta,static_processor=None, dynamic_processor=None, transform=None, ids=None, max_len=None):\n",
    "        self.num_samples = all_masks.shape[0]\n",
    "        self.mask = all_masks\n",
    "        self.value = all_values\n",
    "        self.sta = all_sta\n",
    "        self.transform = transform\n",
    "\n",
    "        self.static_processor = static_processor\n",
    "        self.dynamic_processor = dynamic_processor\n",
    "        self.ids = ids\n",
    "        self.max_len = max_len\n",
    "\n",
    "        self.n_ts = len(self.dynamic_processor['mean'])\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        return self.mask[idx], self.value[idx], self.sta[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "conditional_vars = cols_demo+cols_outcome\n",
    "# These variables are used for the conditional GAN framework\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for i_split in range(N_SPLIT):\n",
    "    \n",
    "    \n",
    "    path2save = PATH_PROCESSED+f'/split{i_split}'\n",
    "    \n",
    "    print(\"path2save:\", path2save)\n",
    "\n",
    "    # loading processed data\n",
    "    df_static = pd.read_csv(path2save+'/df_static.csv', index_col=None)    \n",
    "    df_ts = pd.read_csv(path2save+'/df_ts.csv', index_col=None)\n",
    "    df_static.shape, df_ts.shape\n",
    "\n",
    "    \n",
    "    # merge df_static and df_ts based on RecordID    \n",
    "    df_ts = df_ts.merge(df_static[['RecordID','Label']],on=['RecordID'],how='inner') \n",
    "    df_static = df_static.merge(df_ts[['RecordID']].drop_duplicates(),on=['RecordID'],how='inner') \n",
    "    \n",
    "    # sort both\n",
    "    df_static = df_static.sort_values(by=['RecordID'])\n",
    "    df_ts = df_ts.sort_values(by=['RecordID','Time'])\n",
    "\n",
    "    \n",
    "    df_static.shape, df_ts.shape\n",
    "    \n",
    "\n",
    "    \n",
    "    # load train_ids\n",
    "    with open(path2save+'/train_ids.pkl', 'rb') as f:\n",
    "        train_ids = pickle.load(f)\n",
    "        len(train_ids)\n",
    "\n",
    "    # load dev_ids\n",
    "    # with open(path2save+'dev_ids.pkl', 'rb') as f:\n",
    "    #     dev_ids = pickle.load(f)\n",
    "    dev_ids=[-5555]\n",
    "    len(dev_ids)\n",
    "    \n",
    "    # exclude dev_ids from train_ids\n",
    "    df_static = df_static[~df_static['RecordID'].isin(dev_ids)].copy()\n",
    "    df_ts = df_ts[~df_ts['RecordID'].isin(dev_ids)].copy()\n",
    "    \n",
    "    df_static.shape, df_ts.shape\n",
    "\n",
    "    \n",
    "    \n",
    "    df_static_train, df_static_test, df_ts_train, df_ts_test,   state_preprocess, demo_preprocess  = custom_process(df_static, df_ts, train_ids, state_vars, conditional_vars)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    df_static_train.shape, df_static_test.shape\n",
    "    df_ts_train.shape, df_ts_test.shape\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    demo_vars_enc = demo_preprocess['demo_vars_enc']\n",
    "\n",
    "    # Train dataset\n",
    "    all_masks,all_values,all_sta,all_time_mark = handle_cgan(df_static_train, df_ts_train,state_vars,demo_vars_enc,granularity=GRAN,\n",
    "    IMG_SIZE=IMG_SIZE)\n",
    "\n",
    "    \n",
    "    \n",
    "    ph = Physio3(all_masks,all_values,all_sta,static_processor=demo_preprocess, dynamic_processor=state_preprocess, ids=df_static_train['RecordID'].values, max_len=all_time_mark)  \n",
    "\n",
    "    with open(path2save+f\"/train.pkl\", 'wb') as file:\n",
    "        pickle.dump(ph, file)       \n",
    "\n",
    "    # Val dataset\n",
    "    all_masks,all_values,all_sta,all_time_mark = handle_cgan(df_static_test, df_ts_test,state_vars,demo_vars_enc,granularity=GRAN,IMG_SIZE=IMG_SIZE)\n",
    "    temp2 = all_sta.clone()\n",
    "\n",
    "    \n",
    "    \n",
    "    ph = Physio3(all_masks,all_values,all_sta,static_processor=demo_preprocess,\n",
    "     dynamic_processor=state_preprocess, ids=df_static_test['RecordID'].values, max_len=all_time_mark)    \n",
    "\n",
    "    with open(path2save+f\"/eval.pkl\", 'wb') as file:\n",
    "        pickle.dump(ph, file) \n",
    "\n",
    "    # cgan3 now pixels are from -1 to 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paper2022",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ca5b395ee1d8a1cd2783c3fccc5aaaf2f1d95e614c8b133e284cded792af89cd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
